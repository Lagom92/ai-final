{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Project Sub3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-27c419a1547b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDEFINES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'configs'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from configs import DEFINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋에 데이터 추가하기\n",
    "## http://www.aihub.or.kr/ 에서 데이터를 가져왔다.\n",
    "    - 엑셀 파일의 이름은 변경해야 한다.\n",
    "    - 엑셀의 내용 중 필요한 내용만 따로 빼서 csv파일로 만들었다.\n",
    "    - 길이가 너무 긴 질문이나 답은 제외 시켰다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 열기\n",
    "names = ['academy.xlsx', 'accommodation.xlsx', 'caffe.xlsx', 'cloth.xlsx', 'estate.xlsx', 'food.xlsx', 'play.xlsx', 'service.xlsx', 'store.xlsx']\n",
    "\n",
    "for name in names:\n",
    "    wb = openpyxl.load_workbook(name)\n",
    "    # sheet 열기\n",
    "    sheet = wb['Sheet1']\n",
    "    # 마지막 row 셀 찾기\n",
    "    max_row = sheet.max_row\n",
    "    f = open('ChatBotData.csv', 'a', encoding='utf-8', newline='')\n",
    "    wr = csv.writer(f)\n",
    "    for r in range(2, max_row):\n",
    "        # 질문과 답이 모두 있는것만 csv파일에 넣기\n",
    "        if sheet.cell(row=r, column=13).value and sheet.cell(row=r+1, column=16).value:\n",
    "            q = sheet.cell(row=r, column=13).value\n",
    "            a = sheet.cell(row=r+1, column=16).value\n",
    "            if len(q) < 25 and len(a) < 25:\n",
    "                wr.writerow([q, a, 0])\n",
    "    f.close()\n",
    "    wb.close()\n",
    "\n",
    "\n",
    "names = ['vehicle.xlsx', 'traffic.xlsx', 'waterworks.xlsx', 'passport.xlsx']\n",
    "for name in names:\n",
    "    wb = openpyxl.load_workbook(name)\n",
    "    sheet = wb['Sheet']\n",
    "    max_row = sheet.max_row\n",
    "    f = open('ChatBotData.csv', 'a', encoding='utf-8', newline='')\n",
    "    wr = csv.writer(f)\n",
    "    for r in range(2, max_row):\n",
    "        q = sheet.cell(row=r, column=5).value\n",
    "        a = sheet.cell(row=r, column=6).value\n",
    "        if q and a:\n",
    "            if len(q) < 25 and len(a) < 25:\n",
    "                wr.writerow([q, a, 0])\n",
    "    f.close()\n",
    "    wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분석\n",
    "## wordcloud 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석\n",
    "\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./ChatBotData.csv', encoding='utf-8')\n",
    "\n",
    "q_sentences = list(data['Q'])\n",
    "a_sentences = list(data['A'])\n",
    "\n",
    "# 토크나이즈\n",
    "q_token_sentences = []\n",
    "a_token_sentences = []\n",
    "\n",
    "okt = Okt() # 객체 생성\n",
    "\n",
    "for s in tqdm(q_sentences):\n",
    "    for token, tag in okt.pos(s.replace(' ', '')):\n",
    "        if tag == 'Noun':\n",
    "            q_token_sentences.append(token)\n",
    "            \n",
    "for s in tqdm(a_sentences):\n",
    "    for token, tag in okt.pos(s.replace(' ', '')):\n",
    "        if tag == 'Noun':\n",
    "            a_token_sentences.append(token)\n",
    "\n",
    "# 하나의 문자열로 만들기\n",
    "q_token_sentences = ' '.join(q_token_sentences)\n",
    "a_token_sentences = ' '.join(a_token_sentences)\n",
    "\n",
    "# # jupyter notebook 한글 폰트 \n",
    "# from matplotlib import font_manager, rc\n",
    "# font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# rc('font', family=font_name)\n",
    "\n",
    "# 질문관련 wordcloud\n",
    "q_wordcloud = WordCloud('C:/Windows/Fonts/NanumGothic.ttf').generate(q_token_sentences)\n",
    "\n",
    "plt.imshow(q_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 대답관련 wordcloud\n",
    "a_wordcloud = WordCloud('C:/Windows/Fonts/NanumGothic.ttf').generate(a_token_sentences)\n",
    "\n",
    "plt.imshow(a_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-1. 데이터를 읽고 트레이닝 셋과 테스트 셋으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-1-1. 데이터를 읽고 트레이닝 셋과 테스트 셋으로 분리\n",
    "def load_data(file):\n",
    "    datas = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    q, a = list(datas['Q']), list(datas['A'])\n",
    "    \n",
    "    train_q, test_q, train_a, test_a = train_test_split(q, a, test_size=0.35, random_state=321)\n",
    "    return train_q, train_a, test_q, test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_q, train_a, test_q, test_a = load_data(\"./data_in/ChatBotData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-2. 텍스트 데이터에 정규화를 사용하여 ([~.,!?\\\"':;)(]) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_noise_canceling(sentence):\n",
    "    RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "    sentence = re.sub(RE_FILTER, \"\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-3. 텍스트 데이터에 토크나이징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘 순서\n",
    "    - 텍스트 데이터 prepro_noise_canceling() 함수 처리\n",
    "    - 띄어쓰기 단위로 나누기\n",
    "    - 띄어진 단어들로 벡터 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_data(data):\n",
    "    data = prepro_noise_canceling(data)\n",
    "    data_splited = list(data.split())\n",
    "    \n",
    "    return data_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 내용들이 적용되어야 함\n",
    "    - 텍스트 데이터 prepro_noise_canceling()함수 처리\n",
    "    - 문장을 토큰 단위로 나누기\n",
    "    - dictionary를 활용하여 토큰 인덱스화\n",
    "    - dictionary에 없는 토큰의 경우 UNK 값으로 대체\n",
    "    - 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기\n",
    "    - 기준 문장 길이에 맞게 남은 공간이 있다면 pedding 하기\n",
    "    - 문장 인덱스와 문장 길이 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-2-1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리\n",
    "def enc_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    value = prepro_noise_canceling(value)\n",
    "    \n",
    "    for seq in value:\n",
    "        # 하나의 seq에 index를 저장할 배열 초기화\n",
    "        seq_index =[]\n",
    "\n",
    "        for word in seq.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                # seq_index에 dictionary 안의 인덱스를 extend 한다\n",
    "                seq_index.append(dictionary[word])\n",
    "            else:\n",
    "                # dictionary에 존재 하지 않는 다면 UNK 값을 extend 한다\n",
    "                seq_index.append(dictionary[UNK])\n",
    "\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "        if len(seq_index) > DEFINES.max_sequence_length:\n",
    "            seq_index = seq_index[:DEFINES.max_sequence_length]\n",
    "\n",
    "        # seq의 길이를 저장\n",
    "        seq_len.append(len(seq_index))\n",
    "\n",
    "        # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "        seq_index += (DEFINES.max_sequence_length - len(seq_index)) * [dictionary[PAD]]\n",
    "\n",
    "        # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "        seq_input_index.append(seq_index)\n",
    "\n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-2. 디코더에 필요한 데이터 전 처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 내용들이 적용되어야 함\n",
    "    - 텍스트 데이터 prepro_noise_canceling()함수 처리\n",
    "    - 문장을 토큰 단위로 나누기\n",
    "    - dictionary를 활용하여 토큰 인덱스화\n",
    "    - 첫 인덱스에 STD 추가\n",
    "    - dictionary에 없는 토큰의 경우 UNK 값으로 대체\n",
    "    - 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기\n",
    "    - 기준 문장 길이에 맞게 남은 공간이 있다면 pedding 하기\n",
    "    - 문장 인덱스와 문장 길이 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dec_input_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    value = prepro_noise_canceling(value)\n",
    "\n",
    "    for seq in value:\n",
    "        # 하나의 seq에 index를 저장할 배열 초기화\n",
    "        seq_index =[]\n",
    "\n",
    "        for word in seq.split():\n",
    "            # 디코딩 입력의 처음에는 START가 와야 하므로 STD 값 추가\n",
    "            seq_index.append(dictionary[STD])\n",
    "\n",
    "            if dictionary.get(word) is not None:\n",
    "                # seq_index에 dictionary 안의 인덱스를 extend 한다\n",
    "                seq_index.append(dictionary[word])\n",
    "            else:\n",
    "                # dictionary에 존재 하지 않는 다면 seq_index에 UNK 값을 extend 한다\n",
    "                seq_index.append(dictionary[UNK])\n",
    "\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "        if len(seq_index) > DEFINES.max_sequence_length:\n",
    "            seq_index = seq_index[:DEFINES.max_sequence_length]\n",
    "\n",
    "        # seq의 길이를 저장\n",
    "        seq_len.append(len(seq_index))\n",
    "\n",
    "        # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "        seq_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "\n",
    "        # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "        seq_input_index.append(seq_index)\n",
    "    \n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-3. 디코더에 필요한 데이터 전 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-2-3. 디코더에 필요한 데이터 전 처리 \n",
    "def dec_target_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    value = prepro_noise_canceling(value)\n",
    "\n",
    "    for seq in value:\n",
    "\n",
    "        # 하나의 seq에 index를 저장할 배열 초기화\n",
    "        seq_index =[]\n",
    "\n",
    "        seq_index = [dictionary[word] for word in seq.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "        # END 토큰을 추가 (DEFINES.max_sequence_length 길이를 맞춰서 추가)\n",
    "        \n",
    "        seq_index = seq_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n",
    "\n",
    "        # seq의 길이를 저장\n",
    "        seq_len.append(len(seq_index))\n",
    "\n",
    "        # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "        seq_index += ( DEFINES.max_sequence_length - len(seq_index)) * [dictionary[PAD]]\n",
    "\n",
    "        # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "        seq_input_index.append(seq_index)\n",
    "   \n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input과 output dictionary를 만드는 함수\n",
    "def in_out_dict(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target\n",
    "\n",
    "\n",
    "# 학습에 들어가 배치 데이터를 만드는 함수\n",
    "def train_input_fn(train_input_enc, train_input_dec, train_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은\n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # train_input_enc, train_output_dec, train_target_dec\n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_input_dec, train_target_dec))\n",
    "    # 전체 데이터를 썩는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"train batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을\n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    # 데이터 각 요소에 대해서 in_out_dict 함수를\n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    dataset = dataset.map(in_out_dict)\n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면\n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    dataset = dataset.repeat()\n",
    "    # make_one_shot_iterator를 통해 이터레이터를\n",
    "    # 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 텐서\n",
    "    # 개체를 넘겨준다.\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "# 평가에 들어가 배치 데이터를 만드는 함수\n",
    "def eval_input_fn(eval_input_enc, eval_input_dec, eval_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은\n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # eval_input_enc, eval_input_dec, eval_target_dec\n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_input_dec, eval_target_dec))\n",
    "    # 전체 데이터를 섞는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을\n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    # 데이터 각 요소에 대해서 in_out_dict 함수를\n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    dataset = dataset.map(in_out_dict)\n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면\n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    # 평가이므로 1회만 동작 시킨다.\n",
    "    dataset = dataset.repeat(1)\n",
    "    # make_one_shot_iterator를 통해\n",
    "    # 이터레이터를 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의\n",
    "    # 텐서 개체를 넘겨준다.\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-3-1. 단어 사전 파일 vocabularyData.voc를 생성하고 단어와 인덱스 관계를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-fdf24aa7b622>, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-fdf24aa7b622>\"\u001b[1;36m, line \u001b[1;32m36\u001b[0m\n\u001b[1;33m    with open(DEFINES.vocabulary_path, 'r', encoding='utf-8') as voc_file:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Req 1-3-1. 단어 사전 파일 vocabularyData.voc를 생성하고 단어와 인덱스 관계를 출력\n",
    "def load_voc():\n",
    "    # 사전을 담을 배열 준비한다.\n",
    "    voc_list = []\n",
    "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
    "    # 그 파일의 존재 유무를 확인한다.\n",
    "    if (not (os.path.exists(DEFINES.vocabulary_path))):\n",
    "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
    "        # 데이터를 가지고 만들어야 한다.\n",
    "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
    "        # 데이터 파일의 존재 유무를 확인한다.\n",
    "        \n",
    "        data_df = pd.read_csv(\"./data_in/ChatBotData.csv\")\n",
    "        # 판다스의 데이터 프레임을 통해\n",
    "        # 질문과 답에 대한 열을 가져 온다.\n",
    "        question, answer = data_df[\"Q\"], data_df[\"A\"]\n",
    "        question = prepro_like_morphlized(question)\n",
    "\n",
    "        data = []\n",
    "        # 질문과 답변을 extend을\n",
    "        # 통해서 구조가 없는 배열로 만든다.\n",
    "        data.extend(question)\n",
    "        data.extend(answer)\n",
    "\n",
    "        # data를 토크나이즈하여 words에 저장한다. \n",
    "        words = tokenizing_data(data)\n",
    "        \n",
    "        # 중복되는 단어(토큰)를 제거\n",
    "        words = list(set(words))\n",
    "        \n",
    "        # 데이터 없는 내용중에 MARKER 추가\n",
    "        words[:0] = MARKER\n",
    "        \n",
    "        # 사전 파일을 생성 \n",
    "        # DEFINES.vocabulary_path에 words안에 저장된 가 단어(토큰)들을 한줄 씩 저장\n",
    "        with open(DEFINES.vocabulary_path, 'w', encoding='utf-8') as voc_file:\n",
    "            for word in words:\n",
    "                voc_file.write(word + \"\\n\")\n",
    "           \n",
    "    # 사전 파일에서 단어(토큰)을 가져와 voc_list에 저장\n",
    "    with open(DEFINES.vocabulary_path, 'r', encoding='utf-8') as voc_file:\n",
    "        for line in voc_file:\n",
    "            voc_list.append(line.strip())\n",
    "\n",
    "    # make() 함수를 사용하여 dictionary 형태의 char2idx, idx2char 저장\n",
    "    char2idx, idx2char = make_voc(voc_list)\n",
    "\n",
    "    return char2idx, idx2char, len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-3-2. 사전 리스트를 받아 인덱스와 토큰의 dictionary를 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_voc(voc_list):\n",
    "    char2idx = {}\n",
    "    idx2char = {}\n",
    "    for idx, char in enumerate(voc_list):\n",
    "        char2idx[char] = idx\n",
    "        idx2char[idx] = char\n",
    "    \n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-3-3. 예측용 단어 인덱스를 문장으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-3-3. 예측용 단어 인덱스를 문장으로 변환\n",
    "def pred_next_string(value, dictionary):\n",
    "    string_list = []\n",
    "    \n",
    "    for v in value:\n",
    "        for index in v[\"indexs\"]:\n",
    "            string_list.append(dictionary[index])\n",
    "    \n",
    "    answer = \"\"\n",
    "    \n",
    "    for word in string_list:\n",
    "        if word not in PAD and word not in END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "            \n",
    "    return answer, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask RESTful API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flask restful 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask-restful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('db'):\n",
    "    os.mkdir('db')\n",
    "\n",
    "conn = sqlite3.connect('./db/app.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS thinkB (\n",
    "        question TEXT,\n",
    "        answer TEXT,\n",
    "        create_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )''')\n",
    "\n",
    "c.execute('''Select * FROM thinkB;''')\n",
    "\n",
    "data = c.fetchall() #list\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template\n",
    "from flask_restful import Resource, Api, reqparse\n",
    "import sqlite3\n",
    "\n",
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "\n",
    "def insertDB(q, a):\n",
    "    conn = sqlite3.connect('./db/app.db')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"insert into thinkB (question, answer) values (:question, :answer)\", {'question': q, 'answer': a})\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "parser = reqparse.RequestParser()\n",
    "\n",
    "class Chat(Resource):\n",
    "    def post(self):\n",
    "        parser.add_argument('question', type=str)\n",
    "        parser.add_argument('answer', type=str)\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        _question = args['question']\n",
    "        _answer = args['answer']\n",
    "\n",
    "        insertDB(_question, _answer)\n",
    "        return {'question': args['question'], 'answer': args['answer']}\n",
    "\n",
    "api.add_resource(Chat, '/chat')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat class \n",
    "### flask restful - post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat(Resource):\n",
    "    def post(self):\n",
    "        try:\n",
    "            parser = reqparse.RequestParser()\n",
    "            parser.add_argument('question', type=str)\n",
    "\n",
    "            args = parser.parse_args()\n",
    "\n",
    "            _question = args['question']\n",
    "            _answer = predict(_question)\n",
    "\n",
    "            datas = {\n",
    "                'flag': True,\n",
    "                'answer': _answer\n",
    "            }\n",
    "            \n",
    "            return datas\n",
    "\n",
    "        except Exception as e:\n",
    "            datas = {\n",
    "                'flag': False,\n",
    "                'answer': str(e)\n",
    "            }\n",
    "            return datas\n",
    "    \n",
    "\n",
    "api.add_resource(Declaration, '/Declaration')\n",
    "api.add_resource(Chat, '/chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
